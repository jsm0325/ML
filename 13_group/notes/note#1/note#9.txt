기계학습 12주차

-인공 신경망-
뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델
딥러닝의 핵심 개념
강력하며 확장성이 좋다

대규모 머신러닝 문제를 다루기에 적합하다.

예시 : 구글 이미지, 애플의 시리, 유튜브 알고리즘, 딥마인드의 알파고

다층 퍼셉트론


-인공 신경망 역사-
▪ 1943년 명제논리를 사용해 동물 뇌의 생물학적 뉴런의 간단한 계산 모델을 제시
▪ 1957년 프랭크 로센블래트 퍼셉트론 탄생 – 신경망 기반 인공지능 연구의 부흥기
▪ 인공 신경망 첫번째 침체기
	1969년 마빈 민스키 퍼셉트론은 XOR문제에는 적용할 수 없다는 것을 수학적 증명
	지구상의 그 누구도 다층 퍼셉트론을 학습시킬 방법을 찾아낸 바가 없다
	1970년대 이후 대부분 기업은 R&D의 방향을 통계기술에
	1980년대 산업계에 전문가 시스템이 도입 (통계기반)
▪ 오류 역전파 알고리즘
	1974년 폴 웨어보스가 오류 역전파 알고리즘으로 다층 퍼셉트론을 학습시키는 방법을 박사 학위 논문으로 발표
	1986년 제프리 힌튼 오류 역전파 알고리즘으로 다층 퍼셉트론을 학습시키는데 성공
	1998년 얀 르쿤 CNN(Convolutional Neural Network) 발표
▪ 1990년대 서포트 벡터 머신 같은 다른 강력한 머신러닝 기술 등장
▪ 그레이언트 소실 문제로 학습이 안되는 문제로 2000년대 중반까지 침체기
	해결 - 시그모이드 대신 ReLU 함수 사용, 가중치 초기화 방법 개선
▪ 2012년 ILSVRC의 우승은 알렉스넷(AlexNet)이라 명명된 CNN 기반의 인공 신경망이 차지
▪ 현재 인공 신경망을 기반으로 한 놀라운 제품들이 출시

그레디언트 소실 문제란?
층이 깊게 쌓이면 기울기가 없어져서 학습이 안됨
해결방법 : 시그모이드 대신 ReLU 함수 사용, 가중치 초기화 방법 개선

*인공신경망의 부흥 이유
1. 컴퓨터 성능 개선 2. 대량의 데이터 3. 알고리즘의 개선 


*인공 뉴런 
▪ 생물학적 뉴런에 착안한 매우 단순한 신경망 모델
▪ 하나 이상의 이진(on/off) 입력과 이진 출력 하나를 가짐.
▪ 입력이 일정 개수만큼 활성화되었을 때 출력을 내보냄

이 간단한 인공 뉴런 모델로 인공 뉴런의 네트워크를 만듬

*퍼셉트론 
가장 간단한 인공 신경만 구조로 1957년 프랑크 로젠블라트가 제안 
TLU or LTU라 불리는 인공 뉴런활용

*퍼셉트론 정의 
하나의 층에 여러 개의 TLU로 구성
TLU 각각은 모든 입력과 연결됨

*TLU(threshold logic unit)와 선형 이진 분류
입력값과 가중치를 곱한 값들의 합에 계단함수 적용
하나의 TLU를 간단한 이진분류기로 활용 가능
모든 입력값(특성)의 선형 조합을 계산한 후에 임곗값을 기준으로 양성/음성 분류
작동은 로지스틱 회귀 또는 선형 SVM 분류기와 비슷.
TLU 모델 학습 = 최적의 가중치 𝑤𝑖를 찾기

-퍼셉트론 학습 알고리즘-
오차가 감소되도록 가중치를 조절하며 뉴런 사이의 관계를 강화
하나의 샘플이 입력될 때 마다 예측한 후에 오차를 계산하여 오차가 줄어드는 방향으로 가중치 조절

-퍼셉트론과 선형성-
▪ 각 출력 뉴런의 결정경계가 선형
▪ 복잡한 패턴 학습 못함. 퍼셉트론은 매우 단순한 경우만 해결 가능.
▪ 퍼셉트론 수렴 이론: 선형적으로 구분될 수 있는 모델은 언제나 학습 가능

-다층 퍼셉트론(MLP)-
퍼셉트론을 여러 개 쌓아 올린 인공신경망

입력층 하나와 은닉층이라 불리는 하나 이상의TLU층과 출력층으로 구성

모든 층은 편향을 포함 다음층과 완전히 연결 됨
▪ 다층 퍼셉트론(MLP) 특징
▪ 랜덤하게 설정함. 그렇지 않으면 층의 모든 뉴런이 동일하게 움직임
▪ 활성화 함수: 보통 계단함수 대신에 다른 함수 사용.
• 로지스틱(시그모이드)
• 하이퍼볼릭 탄젠트 함수(쌍곡 탄젠트 함수)
• ReLU 함수

활성화 함수 대체 필요성
▪ 선형성을 벗어나기 위해
	선형 변환을 여러 개 연결 해도 선형 변환에 머무름.
	복잡한 문제 해결 불가능
▪ 비선형 활성화 함수를 충분히 많은 층에서 사용하면 매우 강력한 모델 학습 가능

*완결연결 층 
층에 속한 각각의 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때를 가리킴
여러 개의 완전연결 층으로 구성된 다층 퍼센트론 모델 계산
하나의 층에서 이루어지는 입력과 출력을 행렬 수식으로 표현 가능 


심층 신경망(DNN)
여러 개의 은닉층을 쌓아올린 인공신경망

다층 퍼셉트론은 층을 늘릴수록 더 복잡한 문제를 해결할 수 있지만 훈련과정이 어려워짐
이를 해결한 것이 역전파 훈련 알고리즘

-역전파 알고리즘-
▪ 1단계(정방향): 각 훈련 샘플에 대해 먼저 예측을 만든 후 오차 측정(순전파)
타겟과 예측 아웃풋간의 오차

▪ 2단계(역방향): 역방향으로 각 층을 거치면서 각 출력연결이 오차에 기여한 정도 측정
( 미적분의 연쇄법칙(Chain rule) 적용)

▪ 3단계: 오차가 감소하도록 모든 가중치 조정


-회귀를 위한 다층 퍼셉트론-
▪ 출력 뉴런 수
	예측해야 하는 값의 수에 따라 출력 뉴런 설정
	예제 1: 주택 가격 예측 (출력 뉴런 1개)
	예제 2: 다변량 회귀(동시에 여러값 예측하기)
▪ 활성화 함수 지정
	출력값에 특별한 제한이 없다면 활성화 함수 사용하지 않음.
	출력이 양수인 경우
	ReLU 또는 softplus 사용 가능
▪ 손실함수
	일반적으로 평균제곱오차(MSE) 활용
	이상치가 많을 경우: 평균절댓값오차(MAE) 사용 가능
	후버(Huber) 손실 사용 가능


-분류를 위한 다층 퍼셉트론-
▪ 이진분류
	하나의 출력 뉴런 사용
	활성화 함수: 로지스틱 함수
▪ 다중레이블 이진분류
	다층 퍼셉트론 활용
	예제 1: 이메일의 스팸 여부와 함께 긴급메일 여부 확인 가능
	예제 2: 다중 클래스 분류
	 • MNIST 숫자 이미지. 0부터 9까지
	 • 출력층 활성화 함수: 소프트맥스 함수
▪ 손실함수
	크로스 엔트로피

*케라스 
모든 종류의 신경망을 손쉽게 만들어 주는 최상위 딥러닝 API제공 

*케라스 함수형 API를 사용하여 복잡한 모델 만들기
모든 레이블을 순차적으로 처리하는 것 대신 다양한 신경망 구축을 위해 함수형 API 활용
예로는 와이드&딥 신경망

*와이드&딥 구조
깊게 쌓은 층을 사용한 복잡한 패턴과 짧은 경로를 사용한 간단한 규칙 모두 학습가능한 모델

-서브클래싱 API로 동적 모델 만들기-
동적 모델 생성
정적 모델 vs 동적 모델
정적 모델: 한 번 선언되면 변경할 수 없는 모델. 모델 저장, 복사, 공유가 용이하며 모델의 구조를 출력하고 분석하기 쉽다.
동적 모델: 반복문, 조건문 등을 활용하여 동적으로 모델을 생성할 수 있으며, 명령형 프로그래밍 방식이 요구된다.

서브클래스 API 활용 동적 모델 생성 가능 
Model 클래스 상속
	__init__() 메서드를 활용하여 은닉층과 출력층을 설정한다.
	call() 메서드를 사용하여 층을 동적으로 구성할 수 있다.
	for 반복문, if 조건문, 텐서플로 저수준 연산 등을 활용 가능하다.
단점
	모델 구조가 call() 메서드 안에 숨겨져 있어 케라스가 분석하기 어렵다.
	모델 저장 및 복사가 불가능하다.
	summary() 메서드의 활용이 제한된다.
	층의 목록만 확인 가능하며, 층 간의 연결 정보를 알 수 없다.
	케라스가 타입과 크기를 미리 확인할 수 없어 실수가 발생할 수 있다.
	높은 유연성이 필요하지 않다면 추천되지 않는다.

좋은 모델은 아니라고 생각을 함 


-모델 저장과 복원-

▪ Sequential 모델과 함수형 API 를 사용해서 훈련된 모델 저장
	케라스는 HDF5 포맷을 사용하여, 모델 구조와 층의 모든 파라미터을 저장함.
	옵티마이저도 저장 (하이퍼파라미터와 현재 상태를 포함)
▪ 저장된 모델을 복원
	모델 복원 : load_model() 함수 활용
▪ 주의 사항
	서브클래스 API 에서는 사용 불가
	save_weights(), load_weights() 메서드를 활용하여 모델 파라미터만 저장/복원 가능
	나머지는 수동으로 처리. 예를 들어, pickle 모듈 활용

*콜백함수 
체크포인트 저장시 사용
훈련의 시작/끝/중간에 호출할 객체를 콜백함수를 이용하여 지정 가능 
지정된 객체가 호출되면서 각자의 역할 수행

************ 페어 질문 *****************************************
(서로 질문을 하고 답변을 하면서 진행함 )
각 활성화 함수들의 특징과 어떤 상황에서 어떤 함수쓰는게 좋을지 한번 정리해보기로 하였음

먼저 로지스틱은 입력을 0과 1사이의 값으로 압축하니깐 이진 분류쪽이 적합할 것이라는 의견이 나왔고 
배운 것처럼 층이 깊게 쌓이면 그레디언트 소실문제가 일어나니까 은닉층이 많을 땐 사용하지 않는 것으로 정리함

두 번째로 하이퍼볼릭 탄젠트는 로지스틱 함수와 유사하지만, 출력 범위가 -1과 1로 확장되어 그레디언트 소실 문제를 완화한다는 특징이 있다고 봤는데 
그럼에도 여전히 로지스틱과 비슷한 문제가 일어날 수 있기 때문에 은닉층이 많을 땐 사용하지 않는 것으로 정리

마지막으로 ReLU 함수는 음수 입력을 0으로 만들고 양수 입력은 그대로 반환하는 특징을 가져서 위의 두가지 함수와 달리 
그레디언트 소실 문제를 해결했으므로 은닉층이 있을 땐 이 함수를 사용하는 것으로 정리함

결론적으로 이진분류에는 로지스틱 혹은 하이퍼볼릭 탄젠트를 사용하고 그레디언트 소실 문제가 나타나거나 
은닉층이 많을 때 ReLu를 사용하는 것으로 의견을 정리함

(위 내용은 서로 이야기를 나누며 질문과 답변으로 고민을 했습니다)
*****************************************************************
