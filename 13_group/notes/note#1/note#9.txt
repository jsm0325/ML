-인공 신경망-
뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델
딥러닝의 핵심 개념
강력하며 확장성이 좋다

대규모 머신러닝 문제를 다루기에 적합하다.

예시 : 구글 이미지, 애플의 시리, 유튜브 알고리즘, 딥마인드의 알파고

다층 퍼셉트론


-인공 신경망 역사-
▪ 1943년 명제논리를 사용해 동물 뇌의 생물학적 뉴런의 간단한 계산 모델을 제시
▪ 1957년 프랭크 로센블래트 퍼셉트론 탄생 – 신경망 기반 인공지능 연구의 부흥기
▪ 인공 신경망 첫번째 침체기
	1969년 마빈 민스키 퍼셉트론은 XOR문제에는 적용할 수 없다는 것을 수학적 증명
	지구상의 그 누구도 다층 퍼셉트론을 학습시킬 방법을 찾아낸 바가 없다
	1970년대 이후 대부분 기업은 R&D의 방향을 통계기술에
	1980년대 산업계에 전문가 시스템이 도입 (통계기반)
▪ 오류 역전파 알고리즘
	1974년 폴 웨어보스가 오류 역전파 알고리즘으로 다층 퍼셉트론을 학습시키는 방법을 박사 학위 논문으로 발표
	1986년 제프리 힌튼 오류 역전파 알고리즘으로 다층 퍼셉트론을 학습시키는데 성공
	1998년 얀 르쿤 CNN(Convolutional Neural Network) 발표
▪ 1990년대 서포트 벡터 머신 같은 다른 강력한 머신러닝 기술 등장
▪ 그레이언트 소실 문제로 학습이 안되는 문제로 2000년대 중반까지 침체기
	해결 - 시그모이드 대신 ReLU 함수 사용, 가중치 초기화 방법 개선
▪ 2012년 ILSVRC의 우승은 알렉스넷(AlexNet)이라 명명된 CNN 기반의 인공 신경망이 차지
▪ 현재 인공 신경망을 기반으로 한 놀라운 제품들이 출시

그레디언트 소실 문제란?
층이 깊게 쌓이면 기울기가 없어져서 학습이 안됨
해결방법 : 시그모이드 대신 ReLU 함수 사용, 가중치 초기화 방법 개선


-인공 신경망의 부흥-
컴퓨터의 성능 개선과 대량의 데이터 생성 그리고 알고리즘의 개선을 통해 일어남


인공 뉴런 
▪ 생물학적 뉴런에 착안한 매우 단순한 신경망 모델
▪ 하나 이상의 이진(on/off) 입력과 이진 출력 하나를 가짐.
▪ 입력이 일정 개수만큼 활성화되었을 때 출력을 내보냄

이 간단한 인공 뉴런 모델로 인공 뉴런의 네트워크를 만듬

퍼셉트론

TLU(threshold logic unit)와 선형 이진 분류
입력값과 가중치를 곱한 값들의 합에 계단함수 적용
하나의 TLU를 간단한 이진분류기로 활용 가능
모든 입력값(특성)의 선형 조합을 계산한 후에 임곗값을 기준으로 양성/음성 분류
작동은 로지스틱 회귀 또는 선형 SVM 분류기와 비슷.
TLU 모델 학습 = 최적의 가중치 𝑤𝑖를 찾기

-퍼셉트론 학습 알고리즘-
오차가 감소되도록 가중치를 조절하며 뉴런 사이의 관계를 강화
하나의 샘플이 입력될 때 마다 예측한 후에 오차를 계산하여 오차가 줄어드는 방향으로 가중치 조절

-퍼셉트론과 선형성-
▪ 각 출력 뉴런의 결정경계가 선형
▪ 복잡한 패턴 학습 못함. 퍼셉트론은 매우 단순한 경우만 해결 가능.
▪ 퍼셉트론 수렴 이론: 선형적으로 구분될 수 있는 모델은 언제나 학습 가능

-다층 퍼셉트론(MLP)-
퍼셉트론을 여러 개 쌓아 올린 인공신경망

입력층 하나와 은닉층이라 불리는 하나 이상의TLU층과 출력층으로 구성

모든 층은 편향을 포함 다음층과 완전히 연결 됨
▪ 다층 퍼셉트론(MLP) 특징
▪ 랜덤하게 설정함. 그렇지 않으면 층의 모든 뉴런이 동일하게 움직임
▪ 활성화 함수: 보통 계단함수 대신에 다른 함수 사용.
• 로지스틱(시그모이드)
• 하이퍼볼릭 탄젠트 함수(쌍곡 탄젠트 함수)
• ReLU 함수

활성화 함수 대체 필요성
▪ 선형성을 벗어나기 위해
	선형 변환을 여러 개 연결 해도 선형 변환에 머무름.
	복잡한 문제 해결 불가능
▪ 비선형 활성화 함수를 충분히 많은 층에서 사용하면 매우 강력한 모델 학습 가능

완전연결 층
층에 속한 각각의 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때를 가리킴
	여러 개의 완전연렬 층으로 구성된 다층 퍼셉트론 모델 계산
	하나의 층에서 이루어지는 입력과 출력을 행렬 수식으로 표현 가능

심층 신경망(DNN)
여러 개의 은닉층을 쌓아올린 인공신경망

다층 퍼셉트론은 층을 늘릴수록 더 복잡한 문제를 해결할 수 있지만 훈련과정이 어려워짐
이를 해결한 것이 역전파 훈련 알고리즘

-역전파 알고리즘-
▪ 1단계(정방향): 각 훈련 샘플에 대해 먼저 예측을 만든 후 오차 측정(순전파)
타겟과 예측 아웃풋간의 오차

▪ 2단계(역방향): 역방향으로 각 층을 거치면서 각 출력연결이 오차에 기여한 정도 측정
( 미적분의 연쇄법칙(Chain rule) 적용)

▪ 3단계: 오차가 감소하도록 모든 가중치 조정


-회귀를 위한 다층 퍼셉트론-
▪ 출력 뉴런 수
	예측해야 하는 값의 수에 따라 출력 뉴런 설정
	예제 1: 주택 가격 예측 (출력 뉴런 1개)
	예제 2: 다변량 회귀(동시에 여러값 예측하기)
▪ 활성화 함수 지정
	출력값에 특별한 제한이 없다면 활성화 함수 사용하지 않음.
	출력이 양수인 경우
	ReLU 또는 softplus 사용 가능
▪ 손실함수
	일반적으로 평균제곱오차(MSE) 활용
	이상치가 많을 경우: 평균절댓값오차(MAE) 사용 가능
	후버(Huber) 손실 사용 가능


-분류를 위한 다층 퍼셉트론-
▪ 이진분류
	하나의 출력 뉴런 사용
	활성화 함수: 로지스틱 함수
▪ 다중레이블 이진분류
	다층 퍼셉트론 활용
	예제 1: 이메일의 스팸 여부와 함께 긴급메일 여부 확인 가능
	예제 2: 다중 클래스 분류
	 • MNIST 숫자 이미지. 0부터 9까지
	 • 출력층 활성화 함수: 소프트맥스 함수
▪ 손실함수
	크로스 엔트로피


-케라스로 다층 퍼셉트론 구현-
모든 종류의 신경망을 손쉽게 만들어 주는 최상위 딥러닝 API 제공

-케라스 시퀀셜 API 활용한 이미지 분류-

-서브클래싱 API로 동적 모델 만들기-
동적 모델 생성
정적 모델 vs 동적 모델
정적 모델: 한 번 선언되면 변경할 수 없는 모델. 모델 저장, 복사, 공유가 용이하며 모델의 구조를 출력하고 분석하기 쉽다.
동적 모델: 반복문, 조건문 등을 활용하여 동적으로 모델을 생성할 수 있으며, 명령형 프로그래밍 방식이 요구된다.

서브클래스 API 활용
Model 클래스 상속
	__init__() 메서드를 활용하여 은닉층과 출력층을 설정한다.
	call() 메서드를 사용하여 층을 동적으로 구성할 수 있다.
	for 반복문, if 조건문, 텐서플로 저수준 연산 등을 활용 가능하다.
단점
	모델 구조가 call() 메서드 안에 숨겨져 있어 케라스가 분석하기 어렵다.
	모델 저장 및 복사가 불가능하다.
	summary() 메서드의 활용이 제한된다.
	층의 목록만 확인 가능하며, 층 간의 연결 정보를 알 수 없다.
	케라스가 타입과 크기를 미리 확인할 수 없어 실수가 발생할 수 있다.
	높은 유연성이 필요하지 않다면 추천되지 않는다.



-모델 저장과 복원-

▪ Sequential 모델과 함수형 API 를 사용해서 훈련된 모델 저장
	케라스는 HDF5 포맷을 사용하여, 모델 구조와 층의 모든 파라미터을 저장함.
	옵티마이저도 저장 (하이퍼파라미터와 현재 상태를 포함)
▪ 저장된 모델을 복원
	모델 복원 : load_model() 함수 활용
▪ 주의 사항
	서브클래스 API 에서는 사용 불가
	save_weights(), load_weights() 메서드를 활용하여 모델 파라미터만 저장/복원 가능
	나머지는 수동으로 처리. 예를 들어, pickle 모듈 활용
