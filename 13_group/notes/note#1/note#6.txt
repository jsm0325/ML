앙상블 학습과 랜덤 포레스트

대중의 지혜가 앙상블 학습의 키 아이디어
전문가의 답보다 무작위로 선택된 수천 명의 사람에게 질문하고 모은 답이 낫다.

- 확실히 하나의 좋은 예측보다 여러 사람이 공통적으로 하는 예측이 더 좋은 경우가 있을 것 같음

앙상블은 여러 개의 예측기로 이루어진 그룹
여러 결과를 종합해 예측값을 지정하는 학습이 앙상블 학습
앙상블 방법은 앙상블 학습을 지원하는 학습 알고리즘

직접 투표 : 예측값의 다수결로 예측값 도출
간접 투표 : 예측값의 확률을 고려해 비중을 두고 예측값의 평균으로 예측값 도출

다수의 결과가 옳지 못한 방향으로 가면 어떡하지?

큰 수의 법칙이란 것이 있음
반복 시행하는 횟수가 많거나 표본이 커질수록 수준이 수렴해 비교적 정확한 예측이 가능

표본을 계속 늘리면 결국 예측이 정확해질 수 있음

- 이런 방식이 어울리는 상황이나 데이터들이 있을 것이고 그런 상황에 활용하면 굉장히 효과적인 방법일거라고 생각함

배깅과 페이스팅

알고리즘은 같으나 학습을 다르게 서브셋을 나눠 훈련시킴

배깅 : 중복 허용 샘플링 방식

페이스팅 : 중복 미허용 샘플링 방식

배깅 분류기는 훈련 세트에서 중복을 허용하여 무작위로 100개의 샘플을 선택해서 훈련하는 거임

일반적으로 편향은 키우고 분산은 줄인다고 함

OOB 평가란 배깅 분류기는 샘플을 중복으로 선택하기 때문에 선택이 안되는 샘플들이 생기게 된다. (한 샘플이 중복으로 여러번 선택이 되기 때문에 선택이 안되는 샘플도 생기는 것)이런 선택이 안되는 샘플들을 활용해서 예측기의 성능을 평가하는 것이 바로 OOB 평가이다

-활용되지 않는 샘플들을 활용할 수 있다는 점에서 OOB 평가의 효용성을 느낄 수 있었다.

OOB 평가가 있으면 검증 세트는 필요없는걸까?

도출 결론 : OOB 평가로도 평가하고 검증세트도 활용해서 평가해 비교해보거나 더 정확히 평가할 수 있을 것 같다.

랜덤 포레스트 알고리즘

트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위성을 더 주입함

트리를 더 다양하게 만들고, 편향을 손해보는 대신에 분산을 낮춘다.

편향을 줄여서 좀 더 무작위성을 더해서 트리를 더 다양하게 만들어서 이를 통해 더 좋은 트리를 만들도록 하는 알고리즘인 것 같다.


엑스트라 트리의 노드 분할 방식은 특성과 특성 임곗값 모두 무작위 선택 때문에 속도가 일반 랜덤 포레스트 보다 훨씬 빠름

빠른 이유는 완전 무작위라 분석이나 계산의 과정이 없어져서 인듯함

특성 중요도 : 해당 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는 지를 측정해서 나타내는 값

훈련이 끝나면 특성마다 중요도의 전체 합이 1이 되도록 결과값을 정규화 하는 과정이 있음
-> 100퍼센트를 기준으로 특성의 중요도를 보여주기 위해 정규화를 하는 것으로 보인다.

부스팅
키 아이디어 : 순차적으로 앞의 모델을 보완해나가면서 예측기를 학습시키는 것
부스팅 방법 : 에이다부스트, 그레이디언트 부스팅

에이다부스트란?
잘못 적용된 가중치를 조정하여 새로운 예측기를 추가하는 앙상블 기법
-> 이전 모델이 제대로 학습하지 못했던 훈련 샘플들에 대한 가중치를 더 높이는 방식으로 새로운 모델을 생성함 이를 통해 학습하기 어려운 샘플에 조금씩 더 잘 적응하는 모델이 연속적으로 만들어져 간다.

분류 안된 샘플을 중심으로 학습을 반복해 최적의 모델을 생성하는 방법

에이다 부스트의 알고리즘

1. 가중치가 적용된 에러율 100개 중 20개 = 20퍼

2. 예측기의 가중치
잘못 예측한 것들을 제대로 예측하기 위해 가중치를 조절

3. 샘플의 가중치 업데이트 
잘못 예측한 샘플의 가중치를 업데이트 해서 에러율을 줄여나간다.
잘못 예측한 샘플에 가중치를 더해주는 듯

4. 예측 결과
가중치 합이 가장 큰 클래스


그레이디언트 부스팅
에이다부스트와 동일한 아이디어이지만 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 대해 새로운 예측기를 학습시킴

GBRT 앙상블 훈련
축소 규제 학습률을 낮게 정하면 많은 수의 의사결정나무가 필요하지만 성능이 좋아진다.
조기종료 기법을 활용해서 최적의 결정트리수를 확인한다.

조기종료 기법은 검증오차가 최소가 되는 지점에서 다시 검증오차가 늘어나면 조기 종료해버리는 것

확률적 그레이디언트 부스팅
각 결정트리 훈련에 사용할 훈련 샘플의 비율을 지정하여 학습하는 방법
속도가 빠르고 편향이 높아지지만, 분산은 낮아진다.

XGboost
빠른 속도와 확장성이 있고 이식성이 뛰어나다.
사이킷런과 비슷한 API를 제공하고 조기 종료 등 다양한 기능을 제공한다고 함


스태킹
키 아이디어 : 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련시킬 수 있을지에서 시작

다른점은
이전은 예측한 값을 취합했으나 스태킹은 취합한 최종 모델을 학습시키는 방법
