기계학습 12주차

*인공 신경망 
딥러닝의 핵심 
강력하며 확장성이 좋음 

*인공신경망의 응용 : 대규모 머신러닝 문제 다루기에 적합 

*인공신경망의 부흥 이유
1. 컴퓨터 성능 개선 2. 대량의 데이터 3. 알고리즘의 개선 

*인공 뉴런 
생물학적 뉴런에 착안한 매우 단순한 신경망 모델
하나 이상의 이진입력과 이진 출력 하나를 가짐
입력이 일정 개수만큼 활성화되었을 때 출력을 내보냄 

*퍼셉트론 
가장 간단한 인공 신경만 구조로 1957년 프랑크 로젠블라트가 제안 
TLU or LTU라 불리는 인공 뉴런활용

*TLU 
입력값과 가중치를 곱한 값들의 합에 계단함수 적용.

*TLU와 선형 이진분류
하나의 TLU를 간단한 이진 분류기로 활용 가능
모든 입력값의 선형 조합을 계산한 후에 임계값을 기준으로 양성/음성 분류
작동은 로지스틱 회귀 또는 선형 SVM 분류기와 비슷
TLU 모델 학습 = 최적의 가중치 wi를 찾기

*퍼셉트론 정의 
하나의 층에 여러 개의 TLU로 구성
TLU 각각은 모든 입력과 연결됨

*퍼셉트론 학습 알고리즘 
오차가 감소되도록 가중치를 조절하며 뉴런 사이의 관계를 강화시킴 
하나의 샘플이 입력될 때 마다 예측한 후에 오차를 계산하여 오차가 줄어드는 방향으로 가중치 조절

*퍼셉트론과 선형성 
각 출력 뉴런의 결정경계가 선형
복잡한 패턴 학습 못함. 퍼셉트론은 매우 단순한 경우만 해결 가능
페섭트론 수렴 이론: 선형적으로 구분될 수 있는 모델은 언제나 학습 가능

*다층 퍼센트론 
퍼센트론을 여러 개 쌓아올린 인공신경망
입력층 하나와 은닉층이라 불리는 하나 이상의 TLU층과 출력층으로 구성 
모든 층은 편향을 포함하며, 다음 층과 완전히 연결되어있음 

*완결연결 층 
여러 개의 완전연결 층으로 구성된 다층 퍼센트론 모델 계산
하나의 층에서 이루어지는 입력과 출력을 행렬 수식으로 표현 가능 

*심층신경망 
여러 개의 은닉층을 쌓아올린 인공신경망 

*역적파 훈련 알고리즘 
1단계 (정방향) : 각 훈련 샘플엥 대해 먼저 예측을 만든 후 오차 측정
2단계(역방향): 역방향으로 각 층을 거치면서 각 출력연결이 오차에 기여한 정도 측정
3단계 : 오차가 감소하도록 모든 가중치 조정

*출력 뉴련 수 
예측해야 하는 값의 수에 따라 출력 뉴런 설정 

*활성화 함수 지정 
출력값에 특별한 제한이 없다면 활성화 함수 사용하지 않음

*손실함수 
일반적으로 평균제곱오차 활용
이상치가 많을 경우 : 평균절댓값오차 사용가능 

*케라스 
모든 종류의 신경망을 손쉽게 만들어 주는 최상위 딥러닝 API제공 

*케라스 함수형 API를 사용하여 복잡한 모델 만들기
모든 레이블을 순차적으로 처리하는 것 대신 다양한 신경망 구축을 위해 함수형 API 활용
예로는 와이드&딥 신경망

*와이드&딥 구조
깊게 쌓은 층을 사용한 복잡한 패턴과 짧은 경로를 사용한 간단한 규칙 모두 학습가능한 모델

*서브클래싱 API로 동적 모델 만들기 

*동적 모델 생성 
Sequential 클래스와 함수형 API는 모두 선언적 방식으로 정적임
한번 선언되면 변경할 수 없는 모델 생성
모델 저장, 복사, 공유 용이
모델 구조 출력 및 모델 분석 용이
반복문, 조건문 등을 활용하여 동적 모델을 생성하고자 할 경우 명령형 프로그래밍 방식 요구됨
서브 클래스 API을 활용하여 동적 모델 생성 가능 

좋은 모델은 아니라고 생각을 함 

*모델 저장과 복원 
Sequential 모델과 함수형API를 사용해서 훈련된 모델 저장 
케라스 HDF 포멧을 사용하여, 모델 구조와 층의 모든 파라미터을 저장함
옵티마이저도 저장 

*콜백함수 
체크포인트 저장시 사용
훈련의 시작/끝/중간에 호출할 객체를 콜백함수를 이용하여 지정 가능 
지정된 객체가 호출되면서 각자의 역할 수행






