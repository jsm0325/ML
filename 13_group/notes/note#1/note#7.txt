ch08. 차원 축소

- 차원의 저주
샘플의 특성이 많으면 학습이 매우 어려워진다.

왜 어려워질까?
샘플이 많을수록 계산량이나 서로 영향을 주는게 많아져서 학습이 어려워지는 것 같다.

샘플이 많을수록 차원이 늘어나게 되는데 그만큼 빈 공간이 늘어나게 되는 것과 같아서 학습량이 많이 필요하게 되고 학습하기 어려워진다. 그 빈공간을 채울만큼 데이터를 모으기도 힘들다.

차원을 줄일 때 손실없이 차원을 줄이는 게 중요하다.
이러한 기법을 차원 축소라고 한다.

차원 축소의 특징
- 특성 수를 줄여서 학습 불가능한 문제를 학습 가능한 문제로 만드는 기법
- 훈력 속도가 빨라지지만, 일부 정보가 유실되기 때문에 성능이 저하 될 수 있다.
- 훈련 속도를 높이는 것 외에도 데이터 시각화에 유용하다.



3차원을 넘는 공간은 상상하기 어렵다. 
그래서 시각화 하려면 3차원 아래로 데이터의 차원을 축소하는게 필요한 것 같다.


차원이 커질수록 두 훈련 샘플 사이의 거리가 매우 커짐 -> 과대적합 위험도 상승
데이터가 있는 문제상황에서 데이터의 분포가 모든 차원에 균일하게 퍼져 있지 않음

차원을 감소시키는 두가지 주요 접근법
투영과 매니폴드 학습


투영이란?
n차원에 존재하는 d차원 부분공간을 d차원 공간으로 투영하는 것

예시로 3d공간의 데이터를 평면에 투영시키면 2d 데이터가 된다.

데이터가 스위스 롤 처럼 말려있으면 투영이 놓치는 정보들이 생기는 듯 하다. 그래서 매니폴드 학습이 필요한 것 같다.

투영 기법 알고리즘 - PCA (주성분 분석), 커널 PCA(비선형 투영)



매니폴드 학습이란?
d차원 매니폴드는 국부적으로 d 차원 초평면으로 보일 수 있는 n차원의 입부

매니폴드 기법 알고리즘 - LLE(지역 선형 임베딩)

매니폴드 가정의 문제점
저차원의 매니폴드 공간으로 차원축소를 진행하면 보다 간단한 매니폴드가 된다는 가정과 함께 사용됨.
- 어떤 가정과 함께 사용된다는 사실은 예외가 발생하면 제대로 동작하지 않을 수 있음

암묵적인 가정이 항상 유효하지 않음.
차원 감소는 훈련 속도는 향상되지만 항상 더 나은 솔루션이 아니다.


분산보존이란?
저차원으로 투영할 때 훈련 세트의 분산이 최대로 보존되는 축을 선택해야 한다.
그 이유는 분산이 최대로 보존되어야 정보가 가장 적게 손실된다.
원본 데이터셋과 투영된 데이터 셋 사이의 평균제곱 거리가 최소화 되는 축 선택

▪ 주성분(PC- Principal Component) 축 찾는 법
▪ 첫 번째 주성분: 훈련 세트에서 분산을 최대한 보존하는 축
▪ 두 번째 주성분: 첫 번째 주성분과 수직을 이루면서 분산을 최대한 보존하는 축
▪ 세 번째 주성분: 첫번째, 두번째 주성분과 수직을 이루면서 분산을 최대한 보존하는 축.
▪ 데이터 셋에 있는 차원의 수만큼 네 번째, 다섯 번째 … n 번째 축을 찾음. 


훈련 셋트의 주성분 찾기
▪ SVD(Singular Value Decomposition – 특잇값 분해) 사용
▪ 파이썬 코드 – svd() 함수로 모든 주성분을 구한 후 처음 두 개의 PC를 정의하는 두 개의 단위 벡터 추출

적절한 차원 수 선택하는 법
분산 비율의 합이 95% 정도로 충분히 되도록 차원수 선택 - 성능이 너무 떨어지지 않도록 하는 방법으로 보임
데이터 시각화 목적이면 차원을 2개나 3개로 선택 - 3차원 이상은 시각화가 어렵기 때문에 3개 이하로 선택하는 것으로 보임



PCA - 주성분 분석
키아이디어 - 데이터의 주성분을 찾아서 차원을 줄인다. 가장 잘 보존하는 주성분은 분산이 최대로 보존되는 축

PCA는 압축을 하는데 사용할 수 있다.
95% 분산을 유지하는 만큼 특성개수를 줄이는 방식

랜덤 PCA, 점진적 PCA와 같은 방식이 있음

랜덤 PCA는 주성분에 대한 근삿값을 확률적 알고리즘을 사용해 찾는 방식

점진적 PCA는 훈련 세트를 미니배치로 나눈 후 하나씩 주입 하는 방식 
온라인 학습에 적용 가능하다고 한다.

커멀 PCA
커널 트릭을 적용한 PCA로 복잡한 비선형 투영을 수행한다. 
비지도학습이고 명확한 측정 기준이 없어서 

LLE - 지역선형임베딩

비선형 차원축소 기법으로 추영이 아닌 매이폴드 학습에 의존한다.
키 아이디어 - 가장 가까운 이웃에 얼마나 선형적으로 연관되어 있는지 측정하고 국부적인 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현을 찾고 잡음이 너무 많지 않은 경우 꼬인 매니폴드를 펼치는 데 작동한다.


그밖의 차원 축소 기법
- 이전 학습 랜덤 투영
랜덤한 선형 투영을 사용해 데이터를 저차원 공간으로 투영

- 다차원 스케일링
샘플 간의 거리를 보존하면서 차원을 축소

- Isomap
각 샘플을 가장 가까운 이웃과 연결하는 식으로 그래프를 만들고, 샘플 간의 지오데식리를 유지하면서 차원을 축소

- t-SNE
비슷한 샘플은 가까이 하고 비슷하지 않은 샘플은 멀리 떨어지도록 하면서 차원을 축소하는 기법

- 선형 판별 분석
분류 알고리즘 이지만 훈련 과정에서 클래스 사이를 가장 잘 구분하는 축을 학습하고 이 축은 데이터가 투영되는 초평면을 정의하는 데 사용한다.



@@ 이번 강의의 의문 및 논의할 점 @@



1. 왜 분산이 높으면 정보가 가장 적게 손실될까?

이를 해소하기 위해 먼저 분산의 의미를 더 이해할 필요가 있다고 생각
분산은 데이터가 흩어진 정도를 말한다.

그러면 분산이 높다는 건 데이터가 많이 흩어졌다는 걸 말한다.
흩어졌다는 건 데이터가 다양한 값을 가졌다는 말과 동일하게 볼 수 있다.

데이터가 최대한 다양한 값을 가지는 축을 선택하면 정보를 최대한 많이 반영하는 걸 기대할 수 있는 맥락인 것 같다.

2. 스위스 롤 같은 모양을 띄는 데이터는 어떤 게 있을까?

찾아보니까 실제 데이터 사례에서 이런 모양을 띄는 경우는 드문 것 같다.
학문적인 영역에서 이런 경우가 생길 수도 있으니 그런 모양의 데이터를 생성해서 그런 경우도 처리할 수 있도록 여러 알고리즘을 만든 것 같다.
아니면 일부만 스위스 롤처럼 약간 겹쳐지거나 하는 경향이 있는 데이터를 위해 만들어진 것 같다.

3. 차원의 저주라고 특성이 많아지면 학습이 어려워진다고 하는데 오히려 특성이 늘면 학습이 효과적이어지는 경우는 없을까?

데이터가 부족한 경우가 생긴다고 했으므로 데이터가 충분하면 오히려 차원수를 높였을 때 더 효과적일 수 도 있을 것 같다.

스위스 롤을 봤을 때 오히려 차원이 높아지면 저런 패턴을 볼 수 있으니까 이런 경우엔 차원을 높이면 얻을 수 있는 정보가 있어서 좋을 것 같다.


