기계학습 13주차 

*합성곱 신경망을 사용항 컴퓨터 비전

대뇌에 시각피질 연구에서 출발하여 1980년대부터 이미지인식 분야에서 사용됨.
주요 활용 예제 : 이미지 검색 서비스, 자율주행 자동차, 영상 자동분류 시스템
음성인식, 자연어 처리 등 다양한 분야에서도 활용됨.

*시각 피질 구조
합성곱 신경망(CNN)은 대뇌 시각 피질 연구에서 시작

국부수용장 모델을 모방 
합성곱신경망(CNN)으로 발전 

*합성곱 층 (CNN)
CNN의 가장 중요한 구성 요소는 합성곱 층



******************* 페어 활동 ************************
Q. CNN의 장점은 무엇이며 어디에 활용했을 때 가장 효과적인가 ?

먼저 이미지에 신경망 적용을 하는것이 일반적인 뉴럴 네트워크로는 어렵기 때문에 
합성곱 필터를 사용하는 CNN을 사용하는 것이라고 배워서 이러한 부분과 
파라미터의 개수가 적은 부분이 장점이라고 생각을 했다. 
또 우리는 CNN이 이미지 처리를 가장 효율적으로 할 수 있는 기술이라고 생각했다.
왜냐하면 합성곱 계층으로 특징점을 효과적으로 찾으면서 기존보다 훨씬 적은 수의 가중치로 이미지를 
처리할 수 있기 때문에 과적합의 가능성이 낮아지기 때문이다.

그래서 CNN은 의학적으로 의료 영상에 사용되어진다면 좋은 효율로 사용될 수 있겠다고 
서로 이야기를 했다.
결론적으로 CNN은 이미지 데이터를 처리할 때 가장 효과적인 기술이라고 결론을 냈다.

(서로 이야기하고 토의한 내용입니다)
*****************************************************************

*필터
입력뉴련에 사용될 가중치 역할 수행 
필터의 모양과 크기가 국부수용장의 모양과 크기를 지정함
다양한 필터 사용

*풀링 층
계산량과 메모리 사용량을줄이면서 과대적합의 위험도를 줄여주는 용도로 사용됨 
풀링 층 뉴런은 가중치가 없음 
보폭을 사용하여 차원을 축소시키는 기능 수행

*평균 풀링 층 
풀링커널 구역내의 평균값 활용
maxpool2d대신에 AvgPool2D 사용

*전형적인 CNN구조 
네트워크를 통과하여 진행할수록 이미지는 점점 작아지지만, 합성곱 층 때문에 일반적으로 점점 더 깊어짐
이미지 인식 문제에서 완전 연결 층의 심층 신경망을 사용하지 않는 이유
합성곱 층에 사용하는 커널 크기 

* CNN 구조(1)
케라스 활용 : 패션 MNIST

LeNet-5 
가장 널리 알려진 CNN구조
출력층이 각 뉴런에서 입력 벡터와 가중치 벡터 사이의 유클리드 거리를 출력함 
출력은 이미지가 얼마나 특성 숫자 클래스에 속하는지 측정함.

*AlexNet
2012년 이미지넷 대회에서 우승 
이 구조는 LeNet-5과 비슷하지만 크고 깊음

*특징
-과대 적합을 줄이기 위해 두 가지 규제 기법을 사용 
규제 1 : 드롭 아웃을 50%비율로 적용
규제2 : 처음으로 합성곱 층 위에 풀링 층을 쌓지 않고 바로 합성곱 층끼리 쌓음.

*정규화 : LRN
뉴런의 출력값을 보다 경쟁적으로 만드는 정규화 기법적인 정규화 단계 사용

*GoogLeNet
인셉션 모듈이라는 서브 네트워크 사용

*인셉션 모듈

*VGGNet 구조
ILSVRC 2014년 대회 2등
합성곱 계층과 폴링 계층으로 구성되는 기본적인 CNN

*ResNet
2015년 ILSVRC 2014년 대회 2등
잔차 네트워크 사용 

*잔차 유닛 
많은 층으로 인한 많은 계산을 줄이기 위해 잔차 유닛 활용
RU를 활용한 잔차학습 효과 - 스킵 연결로 인한 보다 수월한 학습 가능

*ResNet 구조
 각 잔차 유닛은 배치 정규화(BN)와 ReLU, 3x3 커널을 사용
 특성맵의 수는 몇 개의 잔차유닛마다 두 배로 늘어남. 
 반면에 뉴런 수를 반씩 줄임(3x3 + 2 사용, 즉, 폭이 2임.)
 입력과 출력의 모양을 맞추기 위해 빨강색 점선 스킵 부분에 폭이 2인 합성곱 활용
 ResNet-34, ResNet-152

* Xception
 2016년에 소개된 GoogLeNet 과 ResNet 모델의 합성 버전
 톱-5 에러율: 3% 정도
 GoogLeNet의 인셉션 모듈 대신 깊이별 분리합성곱 층 사용

* 분리 합성곱 층
 공간별 패턴인식 합성곱 층과 깊이별 패턴인식 합성곱 층을 분리하여 연속적으로 적용
 공간별 패턴인식: 형태 인식 (입력 특성지도마다 한개만 탐색) 
 깊이별 패턴인식: 입, 코, 눈 으로부터 얼굴을 인식하듯 채널 사이의 패턴인식

* Xception 구조
 입력층에 많은 채널(특성지도)가 존재할 경우에만 활용
 보통 두 개 정도의 정상적인 합성곱 층으로 시작한 후에 깊이별 분리합성곱 층 적용
 보다 적은 수의 파라미터, 보다 적은 양의 메모리, 보다 적은 양의 계산 요구되지만, 성능은 더 좋음

* SENet
 2017년 ILSVRC 이미지넷 대회에서 우승
 톱-5 에러율: 2.25%
 GoogLeNet의 인셉션 모듈과 ResNet의 잔차유닛(RU)에 SE block을 추가하여 보다 좋은 성
 발휘

* SE block 기능
 입력된 특성맵을 대상으로 깊이별 패턴 특성 분석
 패턴 특성들을 파악한 후 출력값 보정
 
* SE block 구조
 전역평균 풀링층 -> 밀집층 -> 밀집층
 첫째 밀집 층: 뉴런 수를 16분의 1로 줄임(squeeze)
 특성맵들 사이의 연관성 학습 강요
 둘째 밀집 층: 뉴런 수를 정상화시킴
 학습된 연관성을 이용하여 입력 특성지도를 보정할 가중치 출력
