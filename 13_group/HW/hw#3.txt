1) 기계 학습에서 학습이란 무엇인지를 정리하시오(2점). 
( 가중치, 손실함수가 무엇인지를 정리하고, 데이터, 가중치, 손실함수를 이용하여 학습이 무엇인지를 정리함.) 


머신러닝에서 손실함수란 
예측값과 실제값의 차이를 구하는 기준을 의미하는 것으로 
손실함수는 정답과 예측을 입력으로 받아 실수값 점수를 만드는데, 이 점수가 높을수록 모델이 안좋은 것이다. 낮을 수록 모델의 예측이 좋다고 볼 수 있다.
학습 알고리즘들은 이 손실 함수를 최소화하도록 모델의 파라미터를 조정한다.
이를 통해 모델의 성능을 평가하고 학습하는데 이용하기 때문에 손실함수는 머신러닝 모델 학습에서 필수 구성요소라고 할 수 있다.
또 머신러닝에서 가중치란 각 입력신호가 결과 출력에 미치는 중요도를 조절하는 매개변수이다. 
가중치가 높을 수록 그 입력 신호에 중요도를 높게 배정하게 된다. 그만큼 해당 입력 신호의 영향력이 커지게 된다.
머신러닝에서 말하는 '학습'이란 
훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득 하는 것이며
이 가중치를 조절하여서 손실함수의 함수값이 최소화되도록 학습시키는 것이 학습의 목적이며 좋은 학습이라고 할 수 있다.

풀어서 다르게 말해보면
훈련 데이터에서 어떤 데이터가 결과를 도출하는데 더 중요한지를 기계가 스스로 찾아내 가중치 값을 정해주고 
해당 가중치 값이 효율적인지를 손실함수의 값을 통해 확인해가면서 손실 함수의 값이 최소가 되는 최적의 가중치 매개변수 값을 찾아가는 것을 머신러닝의 학습이라고 할 수 있다.







2) 확률적 경사 하강법의 소스 코드를 분석하시오(2점). (Page 173, 4장 모델 훈련, 첨부 파일 참조)


n_epochs = 50 # n_epochs에서 에포크는 학습의 횟수를 의미한다. 때문에 이 코드는 50번의 학습 횟수를 지정해주는 코드다.
t0, t1 = 5, 50   # 학습 스케줄 하이퍼파라미터 값을 지정해준다.
// 하이퍼 파라미터는 사용자의 입력값으로 정해진 최적의 값이 없어서 여러번 시도해 보면서 데이터와 모델에 맞는 하이퍼 파라미터를 찾아내야 한다.


def learning_schedule(t): # t0, t1(하이퍼 파라미터 값)을 사용해 학습률을 계산하는 함수이다.
    return t0 / (t + t1)    # 계산한 값을 돌려준다. 아래를 보면 t 값으로 epoch * m + i 이 값이 들어가는데 학습을 진행할수록 분모가 커지므로 학습률이 낮아진다는 걸 알 수 있다.

theta = np.random.randn(2,1)  # theta값을 랜덤 초기화 하는 코드로 theta는 가중치 매개변수를 나타낸다.

for epoch in range(n_epochs):                        # 에포크 값만큼 반복하는 코드이다.
    for i in range(m):                                    # m은 데이터 셋에 포함된 데이터 수를 나타낸다. 각 에포크 마다 m 번 반복하게 된다.
        random_index = np.random.randint(m)    # m 값 범위 내부에서 랜덤한 int 수를 할당하는 코드로 데이터 셋에서 무작위 데이터를 선택하는 코드라고 할 수 있다.
        xi = X_b[random_index:random_index+1]  # xi 는 무작위로 선택된 한 개의 샘플이다. random_index:random_index+1는 범위를 나타낸다. 이 범위의 데이터 포인트를 나타낸다고 볼 수 있다.
        yi = y[random_index:random_index+1]     # yi 는 정답 데이터 셋인 y의 정답 샘플이다. 나머지는 마찬가지 이다. 이 둘을 통해 다음 코드에서 비용 함수의 기울기를 계산한다.
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)  # 이 코드를 통해 그래디언트를 계산한다. 여기서 그래디언트가 비용 함수의 기울기이다.
        eta = learning_schedule(epoch * m + i)   # 현재 에포크와 m,i 즉 현재 데이터셋 단계에 따라 learning_schedule함수를 통해 학습률을 계산한다
        theta = theta - eta * gradients              # 계산된 그래디언트와 학습률을 사용해 매개변수 theta를 업데이트 하는 코드이다.

매번 전체 데이터셋을 이용하는 것이 아닌 무작위로 선택된 하나의 데이터 샘플을 사용해 그레디언트를 계산하는 것이 키 아이디어라고 볼 수 있다.

